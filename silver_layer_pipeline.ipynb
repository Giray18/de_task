{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b213a725-493b-4200-a8a5-d66b063530e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Me\n",
    "### This notebook created to apply data transformations for dataframes read from bronze layer`s external tables and save into silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a2359c-1d99-4e34-81ea-5eb50daef3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f05882-dac6-4af2-a095-863237476909",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215b497c-8264-40af-9de3-027350e18f44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Getting Helper Functions from Helper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b43004-253f-487b-9f23-e43c9704baed",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./helper_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4124c640-8607-4b22-94ad-36dde82160e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining Variables to be Used on Further Loops\n",
    "### Variables could be read from helper notebook as well but for ease of readability defining as hard coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f07164-13a0-4bf5-8a7c-fe8d10898e0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## External file location name (Azure Storage Account)\n",
    "external_storage = \"merkletaskstorage\"\n",
    "\n",
    "## External file location name (Azure Blob Container in storage account) for read operation\n",
    "dataframe_list = [\"itemdfraw\",\"eventdfraw\"]\n",
    "\n",
    "## External file location name (Azure Blob Container in storage account) for write operation\n",
    "naming_conversion_dict = {\"itemdfraw\": \"items\", \"eventdfraw\": \"events\"}\n",
    "\n",
    "# Secret scope\n",
    "sas_key_scope = \"BlobStorage4\"\n",
    "\n",
    "# Secret key name\n",
    "sas_key_name = \"BLB_Strg_Access_KEY\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e570e5d-1015-480e-9c74-cf71beba6b68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Ingested Raw Data from Bronze Layer External Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0343ba25-06ac-46e8-bbb1-71da5c506d6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe read from bronze layer pipeline schema read completed. Total (2198, 7) rows & columns loaded into dataframe from itemdfraw table\nDataframe read from bronze layer pipeline schema read completed. Total (853640, 4) rows & columns loaded into dataframe from eventdfraw table\n"
     ]
    }
   ],
   "source": [
    "for container in dataframe_list:\n",
    "    # Create or use existing mount point\n",
    "    mount_point = f\"/mnt/{container}_raw_2\"\n",
    "    already_mounted = any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts())\n",
    "    if not already_mounted:\n",
    "        spark.conf.set(f\"fs.azure.sas.{container}.{external_storage}.blob.core.windows.net\",\n",
    "                       dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name))\n",
    "        \n",
    "        dbutils.fs.mount(\n",
    "            source = f\"wasbs://{container}@{external_storage}.blob.core.windows.net/\",\n",
    "            mount_point = mount_point,\n",
    "            extra_configs = {\n",
    "                f\"fs.azure.sas.{container}.{external_storage}.blob.core.windows.net\": dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name)\n",
    "            }\n",
    "        )\n",
    "    # Reading CSV files from actual mount point per dataframe and assigning to dataframe name\n",
    "    vars()[container] = read_csv_azure_file(mount_point, container)\n",
    "    # print for debug on read dataframe sizes\n",
    "    shape_df = (vars()[container].count(),len(vars()[container].columns))\n",
    "    message = 'Dataframe read from bronze layer pipeline schema read completed. Total {} rows & columns loaded into dataframe from {} table'.format(shape_df,container)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6dd7003-7a45-4073-9851-d1acd929d218",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Flattening Structure Data Type Holding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8f45e0-04ce-4624-9b14-9e3568a94f05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema for JSON value holding data could be parsed by below one line code however for this task I will manually define as below\n",
    "# schema = spark.read.json(eventdfraw.rdd.map(lambda row: row['event.payload'])).schema\n",
    "\n",
    "# Definining required schema for unstructure column - There is only one column exists on eventdfraw dataframe as detected on data profiling activity\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('event_name', StringType(), True),\n",
    "        StructField('platform', StringType(), True),\n",
    "        StructField('parameter_name', StringType(), True),\n",
    "        StructField('parameter_value', StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Flattenning structured column and selecting needed columns\n",
    "eventdfraw = eventdfraw.withColumn(\"event_payload\", from_json(col(\"`event.payload`\"), schema))\\\n",
    "    .select(\n",
    "        col('event_id'),\n",
    "        col('event_time'),\n",
    "        col('user_id'),\n",
    "        col('event_payload.*')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295252af-2418-43e6-bd74-2c703e2938c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Renaming Some Attributes on Dataframes\n",
    "### Based on data profiling activities made on sample data, 2 attributes name will be changed to make it more understandable, dataframe and attiribute names renaming applied are printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a32e9a5-2764-46d3-83cf-1480198954e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renamed attiributes on dataframe eventdfraw, columns ['parameter_name', 'parameter_value'] to ['sub_event_name', 'sub_event_name_value']\n"
     ]
    }
   ],
   "source": [
    "# Looped through all dataframes by calling values from a dict structure on helper notebook in case there will be a lot of transformation this structure can be changed easily and provide a dynamic structure\n",
    "for df_nm in dataframe_list:\n",
    "    # functioned run only if dataframe is in mapping dict structure (called from helper notebook)\n",
    "    if df_nm in mapping_dict.keys():\n",
    "        # Debug print\n",
    "        print(f\"renamed attiributes on dataframe {df_nm}, columns {list(mapping_dict[df_nm].keys())} to {list(mapping_dict[df_nm].values())}\")\n",
    "        # rename columns function called from helper notebook\n",
    "        globals()[df_nm] = rename_columns(globals()[df_nm],df_nm,mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa44cf0d-fb1e-4f45-a90e-f7fe10eda2ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Casting Dataframe Attiributes to Proper Data Type\n",
    "### Casting some attiributes aligned with business requests and to lessen data size volume, applying same structure with previous operation. Calling needed structures from helpers notebook. Casted columns defined in print statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48de631-20bc-4aea-960c-a1a91d35a8b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casted columns on dataframe, itemdfraw,id as type IntegerType\ncasted columns on dataframe, itemdfraw,price as type FloatType\ncasted columns on dataframe, eventdfraw,user_id as type IntegerType\ncasted columns on dataframe, eventdfraw,event_time as type DateType\n"
     ]
    }
   ],
   "source": [
    "# Looped through all dataframes by calling values from a dict structure on helper notebook in case there will be a lot of transformation this structure can be changed easily and provide a dynamic structure\n",
    "for df_nm in dataframe_list:\n",
    "    # functioned run only if dataframe is in casting dict structure (called from helper notebook)\n",
    "    if df_nm in casting_dict.keys():\n",
    "        for col_name in casting_dict[df_nm].keys():\n",
    "            # Debug print\n",
    "            print(f\"casted columns on dataframe, {df_nm},{col_name} as type {casting_dict[df_nm][col_name]}\")\n",
    "            # rename columns function called from helper notebook\n",
    "            dataType = eval(casting_dict[df_nm][col_name])\n",
    "            globals()[df_nm] = globals()[df_nm].withColumn(\n",
    "                col_name,\n",
    "                col(col_name).cast(dataType())\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f27cdc-ecd8-4783-b82f-8a3aa1dfea28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating Partitioning Column on Fact (events dataframe) by event_time Column\n",
    "### Partition level applied as YEAR since all requested views on top_item datamart contains that level of granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc537eb3-1b15-40ad-9c3d-aac8cd40ac3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created partitioned columns on dataframe, eventdfraw,columnevent_time\n"
     ]
    }
   ],
   "source": [
    "# Looped through all dataframes by calling values from a dict structure on helper notebook in case there will be a lot of transformation this structure can be changed easily and provide a dynamic structure\n",
    "for df_nm in dataframe_list:\n",
    "    # runs only if dataframe is in partition dict structure (called from helper notebook)\n",
    "    if df_nm in partition_dict.keys():\n",
    "        for col_name,new_col_func in partition_dict[df_nm].items():\n",
    "            # Debug print\n",
    "            print(f\"created partitioned columns on dataframe, {df_nm},column{col_name}\")\n",
    "            # Saving partition col name into a variable\n",
    "            partition_cols = list(partition_dict[df_nm][col_name].keys())[0]\n",
    "            for new_col, time_func in partition_dict[df_nm][col_name].items():\n",
    "                # By below loop I am calling needed function from F (functions objects)\n",
    "                if hasattr(F, time_func):  # assigning year function to func_time variable\n",
    "                    func_time = getattr(F, time_func)  \n",
    "                    globals()[df_nm] = globals()[df_nm].withColumn(new_col, func_time(col_name))\n",
    "                else:\n",
    "                    print(f\"Function {time_func} not found in pyspark.sql.functions\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce274165-8cf3-4690-9541-3f90178f7473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving Dataframes to Silver Layer Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76df9623-31af-4d52-82b9-ee4ff40d51c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemdfraw items\nDataframes transformed on silver_layer_pipeline. Total (2198, 7) rows & columns loaded into dataframe for items dataframe`s Silver layer schema write\neventdfraw events\nDataframes transformed on silver_layer_pipeline. Total (853640, 8) rows & columns loaded into dataframe for events dataframe`s Silver layer schema write\n"
     ]
    }
   ],
   "source": [
    "for ex_df_name,new_df_name in naming_conversion_dict.items():\n",
    "    print(ex_df_name,new_df_name)\n",
    "    # Saving transformed data frames into silver layer storage as external table by changing their names\n",
    "    new_df_name_str = new_df_name\n",
    "    vars()[new_df_name] = globals()[ex_df_name]\n",
    "    # printing read file size for debug\n",
    "    shape_df = (vars()[new_df_name].count(),len(vars()[new_df_name].columns))\n",
    "    print('Dataframes transformed on silver_layer_pipeline. Total {} rows & columns loaded into dataframe for {} dataframe`s Silver layer schema write'.format(shape_df,new_df_name_str))\n",
    "    # Create or use existing mount point\n",
    "    mount_point = f\"/mnt/{new_df_name_str}_silver\"\n",
    "    already_mounted = any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts())\n",
    "    if not already_mounted:\n",
    "        spark.conf.set(f\"fs.azure.sas.{new_df_name_str}.{external_storage}.blob.core.windows.net\",\n",
    "                       dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name))\n",
    "        \n",
    "        dbutils.fs.mount(\n",
    "            source = f\"wasbs://{new_df_name_str}@{external_storage}.blob.core.windows.net/\",\n",
    "            mount_point = mount_point,\n",
    "            extra_configs = {\n",
    "                f\"fs.azure.sas.{new_df_name_str}.{external_storage}.blob.core.windows.net\": dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name)\n",
    "            }\n",
    "        )\n",
    "    # Writing dataframes to external Azure storage as delta formatted\n",
    "    if ex_df_name == \"eventdfraw\":\n",
    "        vars()[new_df_name].write.format(\"delta\").partitionBy(partition_cols).option(\"delta.columnMapping.mode\", \"name\").mode(\"overwrite\")\\\n",
    "        .option(\"path\", f\"{mount_point}\").saveAsTable(new_df_name_str)\n",
    "    else:\n",
    "        vars()[new_df_name].write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(\"overwrite\")\\\n",
    "        .option(\"path\", f\"{mount_point}\").saveAsTable(new_df_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bceb1147-f23c-43a7-8d23-447208768495",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer_pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
