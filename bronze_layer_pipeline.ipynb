{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc05563e-1a74-4165-b19e-19a79f3a60f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Me\n",
    "### This notebook created to read files from S3 location and save file contents into ingestion layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc2f149-6448-4264-a63f-9fe367d83bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d7ef14-8009-495e-9c48-0e9567cabd4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd724d5-af37-44e4-9971-f52296ddce40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Getting helper functions from helper notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fab52f-d6dd-45cc-8f4d-6f1e34ba8be8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./helper_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509476c0-968d-42bf-b90e-a61b4b508de7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining variables to be used further loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df563bd-3f1e-4772-95d5-f4b2d3963bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## S3 Storage name\n",
    "storage_location_name = \"merkle-de-interview-case-study\"\n",
    "\n",
    "## Created file list as hard coded due to access unavailability to run dbutils.fs.ls command on storage location\n",
    "file_list = [\"item.csv\",\"event.csv\"]\n",
    "\n",
    "## Schema name (Tables to be saved into)\n",
    "schema_name = \"bronze_layer\"\n",
    "\n",
    "## Metastore_name\n",
    "location_name = \"hive_metastore\"\n",
    "\n",
    "## External file location name\n",
    "external_storage = \"merkletaskstorage\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8f39d1-b64e-471c-9d1e-10fbbe07556b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining file names in storage location and dataframe lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd51cad-aa1d-4e50-9e68-8eb31570e3fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating structures to hold dataframe/source file names to be used in write operations\n",
    "df_file_dict = {}\n",
    "for file_name in file_list:\n",
    "    df_file_dict[file_name] = file_name.split(\".\")[0]+\"dfraw\"\n",
    "    \n",
    "print(df_file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98fbc4a2-454b-4701-80ab-9c498ed850b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading source files assign into dataframe and saving as external table to azure blob storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c3749a-657d-4ef1-aef5-0d1cf67f3618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for key,value in df_file_dict.items():\n",
    "    print(key,value)\n",
    "    # Reading csv files and assigning to defined values from df_file_dict as dataframe name / Casting all columns to string type\n",
    "    # vars method used to assign different source file reads to different variable names // can not apply this with only assigning read to 'value'\n",
    "    vars()[value] = cast_to_string(read_csv_file(storage_location_name,value,[key])) \n",
    "    # printing read file size for debug\n",
    "    shape_df = (vars()[value].count(),len(vars()[value].columns))\n",
    "    print('{} files read from s3 bucket completed. Total {} rows & columns loaded into dataframe for {} first layer schema write'.format(f'{\"First_layer_pipeline\"}',shape_df,value))\n",
    "    # Creating mount on storage location per source table for external storage/ Changing setting to SAS key authentication mode\n",
    "    spark.conf.set(f\"fs.azure.sas.{value}.{external_storage}.blob.core.windows.net\",dbutils.secrets.get(scope = \"BlobStorage4\", key = \"BLB_Strg_Access_KEY\"))\n",
    "    dbutils.fs.mount(\n",
    "    source = f\"wasbs://{value}@{external_storage}.blob.core.windows.net/\",\n",
    "    mount_point = f\"/mnt/{value}_raw_2\",\n",
    "    extra_configs = {f\"fs.azure.sas.{value}.{external_storage}.blob.core.windows.net\": dbutils.secrets.get(scope = \"BlobStorage4\", key = \"BLB_Strg_Access_KEY\")}\n",
    "    )\n",
    "    # Writing dataframes to external Azure storage as csv formatted\n",
    "    vars()[value].write.format(\"csv\").mode(\"append\").option(\"mergeSchema\", \"true\").option(\"header\", \"true\").option(\"path\", f\"/mnt/{value}_raw_2\").saveAsTable(f\"{value}\")\n",
    "    #Unmount from storage locations\n",
    "    dbutils.fs.unmount(f\"/mnt/{value}_raw_2\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 762249934805174,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer_pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
