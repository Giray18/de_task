{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc05563e-1a74-4165-b19e-19a79f3a60f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Me\n",
    "### This notebook created to read files from S3 location and save file contents into ingestion layer storage as external table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc2f149-6448-4264-a63f-9fe367d83bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d7ef14-8009-495e-9c48-0e9567cabd4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd724d5-af37-44e4-9971-f52296ddce40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Getting Helper Functions from Helper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fab52f-d6dd-45cc-8f4d-6f1e34ba8be8",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./helper_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509476c0-968d-42bf-b90e-a61b4b508de7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining Variables to be Used on Further Loops\n",
    "### Variables could be read from helper notebook as well but for ease of readability defining as hard coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df563bd-3f1e-4772-95d5-f4b2d3963bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## S3 Storage name\n",
    "storage_location_name = \"merkle-de-interview-case-study\"\n",
    "\n",
    "## Created file list as hard coded due to access unavailability to run dbutils.fs.ls command on storage location\n",
    "file_list = [\"item.csv\",\"event.csv\"]\n",
    "\n",
    "## Schema name (Tables to be saved into)\n",
    "schema_name = \"bronze_layer\"\n",
    "\n",
    "## Metastore_name\n",
    "location_name = \"hive_metastore\"\n",
    "\n",
    "## External file location name (Azure Storage Account)\n",
    "external_storage = \"merkletaskstorage\"\n",
    "\n",
    "# Secret scope\n",
    "sas_key_scope = \"BlobStorage4\"\n",
    "\n",
    "# Secret key name\n",
    "sas_key_name = \"BLB_Strg_Access_KEY\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8f39d1-b64e-471c-9d1e-10fbbe07556b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving File Names in Storage Location and Designated Dataframe Names in a Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd51cad-aa1d-4e50-9e68-8eb31570e3fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating structures to hold dataframe/source file names to be used in write operations\n",
    "df_file_dict = {}\n",
    "for file_name in file_list:\n",
    "    df_file_dict[file_name] = file_name.split(\".\")[0]+\"dfraw\"\n",
    "    \n",
    "print(df_file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98fbc4a2-454b-4701-80ab-9c498ed850b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Source Files, Assign into Dataframe and Saving as External Table into Azure Blob Storage Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c3749a-657d-4ef1-aef5-0d1cf67f3618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for source_name,df_name in df_file_dict.items():\n",
    "    # Reading csv files and assigning to defined values from df_file_dict as dataframe name / Casting all columns to string type\n",
    "    # vars method used to assign different source file reads to different variable names // can not apply this with only assigning a read to 'value'\n",
    "    vars()[df_name] = cast_to_string(read_csv_file(storage_location_name,df_name,[source_name])) \n",
    "    # printing read file size for debug\n",
    "    shape_df = (vars()[df_name].count(),len(vars()[df_name].columns))\n",
    "    print('{} files read from s3 bucket completed. Total {} rows & columns loaded into dataframe for {} bronze layer schema write'.format(f'{\"Bronze_layer_pipeline\"}',shape_df,df_name))\n",
    "    # Create or use existing mount point\n",
    "    mount_point = f\"/mnt/{df_name}_raw_2\"\n",
    "    already_mounted = any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts())\n",
    "    if not already_mounted:\n",
    "        spark.conf.set(f\"fs.azure.sas.{df_name}.{external_storage}.blob.core.windows.net\",\n",
    "                       dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name))\n",
    "        \n",
    "        dbutils.fs.mount(\n",
    "            source = f\"wasbs://{df_name}@{external_storage}.blob.core.windows.net/\",\n",
    "            mount_point = mount_point,\n",
    "            extra_configs = {\n",
    "                f\"fs.azure.sas.{df_name}.{external_storage}.blob.core.windows.net\": dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name)\n",
    "            }\n",
    "        )\n",
    "    # Writing dataframes to external Azure storage as csv formatted\n",
    "    vars()[df_name].write.format(\"csv\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").option(\"quote\", \"\").option(\"header\", \"true\").options(delimiter='|').option(\"path\", f\"/mnt/{df_name}_raw_2\").saveAsTable(f\"{df_name}\") \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 762249934805174,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer_pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
