{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2122ddb3-f804-4275-9301-8c9f4aa1142a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reads csv files from given s3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1babe5c5-3399-434d-8e37-5d6fbb0fa5a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_file(storage_location_name,dataframe_name,file_list = []):\n",
    "    \"\"\"\n",
    "    Creates dataframe based on defined elements data_frame_names list.\n",
    "    Reads files from cloud storage location conditionally by the file type \n",
    "\n",
    "    Args:\n",
    "        dataframe_name (str): The name of the dataframe.\n",
    "        storage_location_name (str): The name of the storage location name.\n",
    "        file_list (list): Name of the files to be read from storage location saved in a list.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A spark dataframe.\n",
    "    \"\"\"\n",
    "    for file_name in file_list:\n",
    "        if \"csv\" in file_name:\n",
    "            dataframe_name = (spark.read.format(\"csv\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"quote\",'\"')\\\n",
    "            .option(\"escape\",'\"')\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .load(f\"s3a://{storage_location_name}/de/{file_name}\"))\n",
    "        else:\n",
    "            print(f\"No CSV files exists in {storage_location_name} cloud storage location\")\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0ddea7-af92-4eac-99cb-6b485a7dc3fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read csv from Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93849c5f-5e92-4af3-bcb1-c8c1d7517d61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_azure_file(mount_point,dataframe_name):\n",
    "    \"\"\"\n",
    "    Creates dataframe by reading csv files from azure blob storage.\n",
    "\n",
    "    Args:\n",
    "        mount_point (str): mount point specific to container.\n",
    "        dataframe_name (str): The name of the designated dataframe name.\n",
    "        file_list (list): Name of the files to be read from storage location saved in a list.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A spark dataframe.\n",
    "    \"\"\"\n",
    "    dataframe_name = (spark.read.format(\"csv\")\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .option(\"quote\",' ')\\\n",
    "    .option(\"escape\",'\"')\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \"|\")\\\n",
    "    .load(f\"{mount_point}\"))\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b3972d-2e3e-466c-b880-3dc29232d6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Delta file from Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7722af1d-f46c-4641-b16a-4103fe54cfc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_delta_azure_file(mount_point,dataframe_name):\n",
    "    \"\"\"\n",
    "    Creates dataframe by reading delta files from azure blob storage.\n",
    "\n",
    "    Args:\n",
    "        mount_point (str): mount point specific to container.\n",
    "        dataframe_name (str): The name of the designated dataframe name.\n",
    "        file_list (list): Name of the files to be read from storage location saved in a list.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A spark dataframe.\n",
    "    \"\"\"\n",
    "    dataframe_name = (spark.read.format(\"delta\")\\\n",
    "    .load(f\"{mount_point}\"))\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31811ab-0383-423f-89e7-3e31f3a43812",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Casts all columns into string with col method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2346b64e-62b4-4b14-96b8-8a811967fba3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_to_string(df):\n",
    "    from pyspark.sql.functions import col\n",
    "    \"\"\"\n",
    "    Casts columns of tabular dataframe to string format \n",
    "\n",
    "    Args:\n",
    "        dataframe_name (df): dataframe in dataframe format.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: With all columns casted to string\n",
    "    \"\"\"\n",
    "    df = df.select([col(f\"`{c}`\").cast(\"string\") for c in df.columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "892d9ba4-ec01-4901-852e-3ba8debedc1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Casting attiributes mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c38095f-efa7-4e11-9654-0cd8ef7ccc49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "casting_dict = {\"eventdfraw\" : {\"user_id\":\"IntegerType\", \"event_time\":\"DateType\"}, \"itemdfraw\":{\"id\":\"IntegerType\", \"price\":\"FloatType\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba477bc9-0670-4d18-be7b-0b6c6b26c442",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Renaming attiributes mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac0ce4c-4c13-42d6-9d87-324d42dc4a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapping_dict = {\"eventdfraw\" : {\"parameter_name\":\"sub_event_name\", \"parameter_value\":\"sub_event_name_value\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b4ddcb-54ad-44ec-8d5a-e8fb06eb9cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Rename dataframe attributes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888c3a68-80fe-4a02-ada9-3260c34baed0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns(dataframe_name,df_name,mapping_dict = {}):\n",
    "    \"\"\"\n",
    "\t    Renames the columns of a PySpark DataFrame based on the provided configuration. All Notebook names are defined in namespace of related config file section\n",
    "\t    e.g. F_MES_ACTIVITIES_COLUMN_RENAME:\n",
    "                PLANT:PLANT>\n",
    "                DESCRIPTION:OPERATION>\n",
    "\t    Args:\n",
    "\t        DataFrame (pyspark.sql.DataFrame): The DataFrame to rename columns.\n",
    "            df_name (str): String name of dataframe to call correct nested dict item\n",
    "\t    Returns:\n",
    "\t        pyspark.sql.DataFrame: The DataFrame with renamed columns.\n",
    "\t    Example:\n",
    "\t        >>> dataframe = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")], [\"id\", \"name\"])\n",
    "\t        >>> rename_columns_with_config_file(dataframe, config)\n",
    "\t        DataFrame[user_id: int, user_name: string]\n",
    "    \"\"\"\n",
    "    for key,value in mapping_dict[f\"{df_name}\"].items():\n",
    "        dataframe_name = dataframe_name.withColumnRenamed((key),(value))\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a90a396-343e-473c-9e1f-45a3852b820d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Partition column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "713c2bb5-058b-4c3d-8668-fbabf2b32495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partition_dict = {\"eventdfraw\" : {\"event_time\": {\"event_time_year\" :\"year\"}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9ee9a16-36cf-4802-835e-7045886a28fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save dataframe into hive metastore schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7aa48ca-3739-4fd4-a352-be2549b7136e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schema(schema_name,location_name):\n",
    "    \"\"\"\n",
    "    Checks if a schema exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        location_name (str): catalog name to save schema on\n",
    "        schema_name (str): The name of the schema to check.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Empty DF creates schema on defined catalog if not exits\n",
    "    \"\"\"\n",
    "    return spark.sql(f\"CREATE SCHEMA IF NOT EXISTS  {location_name}.{schema_name}\")\n",
    "\n",
    "\n",
    "def check_if_table_exists(schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Checks if a table exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the table to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return spark.catalog.tableExists(f\"hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")\n",
    "\n",
    "def write_to_managed_table(df, table_name, schema_name, location_name, partition_cols = [] ,mode = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a managed table in Delta Lake.\n",
    "\n",
    "    If the table exists and mode is overwrite, it performs an overwrite operation.\n",
    "    Otherwise, it either creates a new table or appends transactions to table based on the `mode` parameter.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The DataFrame to write to the table.\n",
    "        table_name (str): The name of the target table.\n",
    "        schema_name (str): The schema name of target table.\n",
    "        location_name (str): Catalog name to save schema on\n",
    "        mode (str, optional): The write mode.\n",
    "    \"\"\"\n",
    "    #create schema if not exists\n",
    "    create_schema(schema_name,location_name)\n",
    "    # check if the table exists\n",
    "    if check_if_table_exists(schema_name, table_name):\n",
    "        print(f\"Table exists on hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")\n",
    "        if mode == \"overwrite\":\n",
    "            print(f\"Overwriting all transactions on managed table hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")\n",
    "            df.write.format(\"delta\").partitionBy(partition_cols).option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")\n",
    "        else:\n",
    "            print(f\"Appending all transactions on managed table hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")\n",
    "            df.write.format(\"delta\").partitionBy(partition_cols).option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")\n",
    "    else:\n",
    "        print(f\"Writing to managed table hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")\n",
    "        df.write.format(\"delta\").partitionBy(partition_cols).option(\"delta.columnMapping.mode\", \"name\").saveAsTable(f\"hive_metastore.{schema_name}.{table_name}_gold_layer_managed_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "helper_notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
