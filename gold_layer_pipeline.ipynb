{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38849858-e599-4ed7-801d-5a1f74b0e063",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Me\n",
    "### This notebook created to create \"top_item\" data mart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b3895f-7c84-4588-8007-5ca513927f59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3522aa-fbaf-4da9-ab1f-110ea64fe384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, dense_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e9854e-57c1-4ff0-aee9-6af4a64389fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Getting Helper Functions from Helper Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ced1053-6a98-4a19-ae94-1438912bf81d",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./helper_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952f45ed-04de-492d-9784-b4fa735bb340",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining Variables to be Used on Further Loops\n",
    "### Variables could be read from helper notebook as well but for ease of readability defining as hard coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a439dfe5-3356-4bcf-9093-28a6d2c32959",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Schema name (Tables to be saved into)\n",
    "schema_name = \"gold_layer\"\n",
    "\n",
    "## Metastore_name\n",
    "catalog_name = \"hive_metastore\"\n",
    "\n",
    "## External file location name (Azure Storage Account)\n",
    "external_storage = \"merkletaskstorage\"\n",
    "\n",
    "## External file location name (Azure Blob Container in storage account) for read operation\n",
    "dataframe_list = [\"items\",\"events\"]\n",
    "\n",
    "## External file location name (Azure Blob Container in storage account) for write operation\n",
    "naming_conversion_dict = {\"items\": \"dimitems\", \"events\": \"factevents\", \"event_view_item_by_year\" : \"eventviewitembyyear\", \"item_view_numbers_by_year\" : \"itemviewnumbersbyyear\", \"most_used_platform_by_year_rank\" : \"mostusedplatformbyyearrank\"}\n",
    "\n",
    "# Secret scope\n",
    "sas_key_scope = \"BlobStorage4\"\n",
    "\n",
    "# Secret key name\n",
    "sas_key_name = \"BLB_Strg_Access_KEY\"\n",
    "\n",
    "# Partition col names\n",
    "partition_cols = 'event_time_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2a2a59-1654-4849-b698-9c73d0633a75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Ingested Raw Data from Silver Layer External Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d752173-caea-45d6-9bc1-c1904c7296d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe read from silver layer pipeline schema read completed. Total (2198, 7) rows & columns loaded into dataframe from items table\nDataframe read from silver layer pipeline schema read completed. Total (853640, 8) rows & columns loaded into dataframe from events table\n"
     ]
    }
   ],
   "source": [
    "for container in dataframe_list:\n",
    "    # Create or use existing mount point\n",
    "    mount_point = f\"/mnt/{container}_raw_2\"\n",
    "    already_mounted = any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts())\n",
    "    if not already_mounted:\n",
    "        spark.conf.set(f\"fs.azure.sas.{container}.{external_storage}.blob.core.windows.net\",\n",
    "                       dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name))\n",
    "        \n",
    "        dbutils.fs.mount(\n",
    "            source = f\"wasbs://{container}@{external_storage}.blob.core.windows.net/\",\n",
    "            mount_point = mount_point,\n",
    "            extra_configs = {\n",
    "                f\"fs.azure.sas.{container}.{external_storage}.blob.core.windows.net\": dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name)\n",
    "            }\n",
    "        )\n",
    "    # Reading delta files from actual mount point per dataframe and assigning to dataframe name\n",
    "    vars()[container] = read_delta_azure_file(mount_point, container)\n",
    "    # print for debug on read dataframe sizes\n",
    "    shape_df = (vars()[container].count(),len(vars()[container].columns))\n",
    "    message = 'Dataframe read from silver layer pipeline schema read completed. Total {} rows & columns loaded into dataframe from {} table'.format(shape_df,container)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dba6827-dcd2-444d-a351-97e74e557c16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating Requested Views on Top Item Datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75402a1-8889-4e23-905f-699086afffb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Total number of item views in a particular year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a7d8d7-9df7-43f3-b981-d5d4d8efca36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtering events dataframe by event_name col. for view_item events\n",
    "events_view_item = events.filter(events.event_name == \"view_item\")\n",
    "\n",
    "# Inner join between events_view_item and items table on item id column to filter item related view items\n",
    "event_item_join_df = events_view_item.join(items,events_view_item.sub_event_name_value == items.id)\n",
    "\n",
    "# Selecting only needed columns\n",
    "event_item_join_df_filter = event_item_join_df.select(\"event_time_year\",\"id\")\n",
    "\n",
    "# Group by on year column and counting rows to get total views on a year\n",
    "event_view_item_by_year = event_item_join_df_filter.groupBy(\"event_time_year\").count()\n",
    "\n",
    "# Column aliasing\n",
    "event_view_item_by_year = event_view_item_by_year.select(col(\"event_time_year\").alias(\"Year\"),col(\"count\").alias(\"Total Item View\"))\n",
    "\n",
    "# Ordering by Year as descending\n",
    "event_view_item_by_year = event_view_item_by_year.orderBy(col(\"Year\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baad9ad2-a947-473f-bb7e-3e9781257605",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Rank of an item based on number of views in a particular year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19251614-a3a8-4c74-9376-df286a82a590",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gathered only needed cols for calculation (item id taken since it is unique per item)\n",
    "event_item_join_df_filter = event_item_join_df.select(\"event_time_year\",\"id\")\n",
    "\n",
    "# Grouped by year and id cols and counted rows\n",
    "year_item_group_by = event_item_join_df.groupBy(\"event_time_year\",\"id\").count()\n",
    "\n",
    "# Creating window item which will partition count in a year level (start again when year changes)\n",
    "window_item = Window.partitionBy(year_item_group_by[\"event_time_year\"]).orderBy(desc(\"count\"))\n",
    "\n",
    "# Rank column added and window function applied\n",
    "item_view_numbers_by_year = year_item_group_by.withColumn(\"item_views\",dense_rank().over(window_item))\n",
    "\n",
    "# Column aliasing\n",
    "item_view_numbers_by_year = item_view_numbers_by_year.select(col(\"event_time_year\").alias(\"Year\"),col(\"count\").alias(\"Total Item View\"),col(\"id\").alias(\"Item Id\"),col(\"item_views\").alias(\"View Ranking\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6aaaa84-a31e-44c1-9a98-10970559588d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The most used platform in particular year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd3bb37-a69e-4254-92e0-7cfe2875e0dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "event_item_join_df_filter = event_item_join_df.select(\"event_time_year\",\"platform\")\n",
    "\n",
    "most_used_platform_by_year = event_item_join_df_filter.groupBy(\"event_time_year\",\"platform\").count()\n",
    "\n",
    "window_item = Window.partitionBy(most_used_platform_by_year[\"event_time_year\"]).orderBy(desc(\"count\"))\n",
    "\n",
    "most_used_platform_by_year_rank = most_used_platform_by_year.withColumn(\"platform_usage\",dense_rank().over(window_item))\n",
    "\n",
    "most_used_platform_by_year_rank = most_used_platform_by_year_rank.filter(most_used_platform_by_year_rank.platform_usage == 1)\n",
    "\n",
    "# Column aliasing\n",
    "most_used_platform_by_year_rank = most_used_platform_by_year_rank.select(col(\"event_time_year\").alias(\"Year\"),col(\"platform\").alias(\"Most Used Platform\"),col(\"count\").alias(\"Total Platform Usage\"))\n",
    "\n",
    "# Ordering by Year as descending\n",
    "most_used_platform_by_year_rank = most_used_platform_by_year_rank.orderBy(col(\"Year\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8993831a-d504-454f-82d9-9d2a7c1a76d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving dataframes to gold layer storage\n",
    "### For this layer dataframes will also be saved as managed table addition to external table (For proving capability on this option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5d5c1f-c91d-43ea-a11a-5be1fe4fc676",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### External table on external location save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426a50a0-e7ef-4d6f-b5a7-bc773dce7504",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items dimitems\nDataframes read on gold_layer_pipeline. Total (2198, 7) rows & columns loaded into dataframe for dimitems dataframe`s gold layer schema write\nevents factevents\nDataframes read on gold_layer_pipeline. Total (853640, 8) rows & columns loaded into dataframe for factevents dataframe`s gold layer schema write\nevent_view_item_by_year eventviewitembyyear\nDataframes read on gold_layer_pipeline. Total (6, 2) rows & columns loaded into dataframe for eventviewitembyyear dataframe`s gold layer schema write\nitem_view_numbers_by_year itemviewnumbersbyyear\nDataframes read on gold_layer_pipeline. Total (13093, 4) rows & columns loaded into dataframe for itemviewnumbersbyyear dataframe`s gold layer schema write\nmost_used_platform_by_year_rank mostusedplatformbyyearrank\nDataframes read on gold_layer_pipeline. Total (6, 3) rows & columns loaded into dataframe for mostusedplatformbyyearrank dataframe`s gold layer schema write\n"
     ]
    }
   ],
   "source": [
    "for ex_df_name,new_df_name in naming_conversion_dict.items():\n",
    "    print(ex_df_name,new_df_name)\n",
    "    # Saving transformed data frames into silver layer storage as external table by changing their names\n",
    "    new_df_name_str = new_df_name\n",
    "    vars()[new_df_name] = globals()[ex_df_name]\n",
    "    # printing read file size for debug\n",
    "    shape_df = (vars()[new_df_name].count(),len(vars()[new_df_name].columns))\n",
    "    print('Dataframes read on gold_layer_pipeline. Total {} rows & columns loaded into dataframe for {} dataframe`s gold layer schema write'.format(shape_df,new_df_name_str))\n",
    "    # Create or use existing mount point\n",
    "    mount_point = f\"/mnt/{new_df_name_str}_silver\"\n",
    "    already_mounted = any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts())\n",
    "    if not already_mounted:\n",
    "        spark.conf.set(f\"fs.azure.sas.{new_df_name_str}.{external_storage}.blob.core.windows.net\",\n",
    "                       dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name))\n",
    "        \n",
    "        dbutils.fs.mount(\n",
    "            source = f\"wasbs://{new_df_name_str}@{external_storage}.blob.core.windows.net/\",\n",
    "            mount_point = mount_point,\n",
    "            extra_configs = {\n",
    "                f\"fs.azure.sas.{new_df_name_str}.{external_storage}.blob.core.windows.net\": dbutils.secrets.get(scope = sas_key_scope, key = sas_key_name)\n",
    "            }\n",
    "        )\n",
    "    # Writing dataframes to external Azure storage as delta formatted\n",
    "    if ex_df_name == \"events\":\n",
    "        vars()[new_df_name].write.format(\"delta\").partitionBy(partition_cols).option(\"delta.columnMapping.mode\", \"name\").mode(\"overwrite\")\\\n",
    "        .option(\"path\", f\"{mount_point}\").saveAsTable(new_df_name_str)\n",
    "    else:\n",
    "        vars()[new_df_name].write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(\"overwrite\")\\\n",
    "        .option(\"path\", f\"{mount_point}\").saveAsTable(new_df_name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12c44f8-636f-476c-9c13-2e0e908740cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving dataframes as managed table on hive metastore`s gold layer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bd8300-81ed-4bd9-bdc4-cadbf57d9118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items dimitems\nDataframes read on gold_layer_pipeline. Total (2198, 7) rows & columns loaded into dataframe for dimitems dataframe`s gold layer schema write\nTable exists on hive_metastore.gold_layer.dimitems_gold_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.gold_layer.dimitems_gold_layer_managed_table\nevents factevents\nDataframes read on gold_layer_pipeline. Total (853640, 8) rows & columns loaded into dataframe for factevents dataframe`s gold layer schema write\nTable exists on hive_metastore.gold_layer.factevents_gold_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.gold_layer.factevents_gold_layer_managed_table\nevent_view_item_by_year eventviewitembyyear\nDataframes read on gold_layer_pipeline. Total (6, 2) rows & columns loaded into dataframe for eventviewitembyyear dataframe`s gold layer schema write\nTable exists on hive_metastore.gold_layer.eventviewitembyyear_gold_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.gold_layer.eventviewitembyyear_gold_layer_managed_table\nitem_view_numbers_by_year itemviewnumbersbyyear\nDataframes read on gold_layer_pipeline. Total (13093, 4) rows & columns loaded into dataframe for itemviewnumbersbyyear dataframe`s gold layer schema write\nTable exists on hive_metastore.gold_layer.itemviewnumbersbyyear_gold_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.gold_layer.itemviewnumbersbyyear_gold_layer_managed_table\nmost_used_platform_by_year_rank mostusedplatformbyyearrank\nDataframes read on gold_layer_pipeline. Total (6, 3) rows & columns loaded into dataframe for mostusedplatformbyyearrank dataframe`s gold layer schema write\nTable exists on hive_metastore.gold_layer.mostusedplatformbyyearrank_gold_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.gold_layer.mostusedplatformbyyearrank_gold_layer_managed_table\n"
     ]
    }
   ],
   "source": [
    "for ex_df_name,new_df_name in naming_conversion_dict.items():\n",
    "    print(ex_df_name,new_df_name)\n",
    "    # Saving created views and data frames into gold layer storage as managed table by changing their names\n",
    "    new_df_name_str = new_df_name\n",
    "    vars()[new_df_name] = globals()[ex_df_name]\n",
    "    # printing read file size for debug\n",
    "    shape_df = (vars()[new_df_name].count(),len(vars()[new_df_name].columns))\n",
    "    print('Dataframes read on gold_layer_pipeline. Total {} rows & columns loaded into dataframe for {} dataframe`s gold layer schema write'.format(shape_df,new_df_name_str))\n",
    "    # writing dataframes by using function gathered from helper notebook\n",
    "    write_to_managed_table(vars()[new_df_name],new_df_name_str,schema_name,catalog_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e06aaef-104a-4b42-8dcc-b94843d30177",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_layer_pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
